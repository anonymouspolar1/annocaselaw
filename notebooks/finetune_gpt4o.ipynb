{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import tiktoken # for token counting\n",
    "from collections import defaultdict\n",
    "\n",
    "from src.prompts import *\n",
    "from src.hapi_elpers import sample_cases, clear_openai_cache\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataPreparer():\n",
    "    def __init__(self):\n",
    "        self.training_data = None\n",
    "        self.validation_data = None\n",
    "\n",
    "    def prepare_data(self, data_paths: list[str], annotation_types: dict = annotation_types):\n",
    "        \"\"\"Prepare data in the format required by OpenAI's fine-tuning API\"\"\"\n",
    "        data = []\n",
    "        \n",
    "        for case_path in data_paths:\n",
    "            with open(case_path, encoding='utf-8') as f:\n",
    "                file = json.load(f)\n",
    "            \n",
    "            prompt = f\"\"\"Annotate the below law case file according to the following 5 annotation types:\n",
    "            {json.dumps(annotation_types, indent=2)}\n",
    "\n",
    "            Take your time, be as thorough as possible, and combine all the annotations from a single annotation type into a list of comma-separated strings. Do not include sources. Annotations must be direct, unedited quotes from the case file.\n",
    "\n",
    "            Always respond to the user in JSON format where the keys are the annotation types, and the value for each key is an array (list) of strings where each string is a separate annotation relevant to the given key. Include no other text in your response.\n",
    "\n",
    "            Law case file to annotate:\n",
    "            {json.dumps(file['text'], indent=2)}\"\"\"\n",
    "\n",
    "            response = json.dumps(file['annotations'], indent=2)\n",
    "\n",
    "            # Create the training example\n",
    "            training_example = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": response}\n",
    "                ]\n",
    "            }\n",
    "            data.append(training_example)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def save_data(self, data_id: str):\n",
    "        \"\"\"Save training data in JSONL format as required by OpenAI\"\"\"\n",
    "        if self.training_data is None or self.validation_data is None:\n",
    "            print(\"Prepare training and validation data before saving\")\n",
    "            return\n",
    "        \n",
    "        if not os.path.exists(f\"../data/finetuning/{data_id}\"):\n",
    "            os.makedirs(f\"../data/finetuning/{data_id}\")\n",
    "        \n",
    "        training_save_path = f\"../data/finetuning/{data_id}/train.jsonl\"\n",
    "        with open(training_save_path,'w') as f:\n",
    "            for example in self.training_data:\n",
    "                f.write(json.dumps(example) + '\\n')\n",
    "\n",
    "        validation_save_path = f\"../data/finetuning/{data_id}/val.jsonl\"\n",
    "        with open(validation_save_path,'w') as f:\n",
    "            for example in self.validation_data:\n",
    "                f.write(json.dumps(example) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Finetuner():\n",
    "    def __init__(self, model: str):\n",
    "        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        self.model = model\n",
    "        self.training_file_id = None\n",
    "        self.validation_file_id = None\n",
    "        self.job_id = None\n",
    "        self.finetuned_model = None\n",
    "\n",
    "    def upload_files(self, data_id: str) -> str:\n",
    "        \"\"\"Upload training file to OpenAI\"\"\"\n",
    "        with open(f\"../data/finetuning/{data_id}/train.jsonl\", \"rb\") as f:\n",
    "            response = self.client.files.create(\n",
    "                file=f,\n",
    "                purpose=\"fine-tune\"\n",
    "            )\n",
    "        self.training_file_id = response.id\n",
    "        print(f\"Training file uploaded successfully. File ID: {self.training_file_id}\")\n",
    "\n",
    "        with open(f\"../data/finetuning/{data_id}/val.jsonl\", \"rb\") as f:\n",
    "            response = self.client.files.create(\n",
    "                file=f,\n",
    "                purpose=\"fine-tune\"\n",
    "            )\n",
    "        self.validation_file_id = response.id\n",
    "        print(f\"Validation file uploaded successfully. File ID: {self.validation_file_id}\")\n",
    "\n",
    "\n",
    "    def create_job(self, n_epochs: int = 5) -> str:\n",
    "        \"\"\"Create and start a fine-tuning job\"\"\"\n",
    "        if self.training_file_id is None:\n",
    "            raise ValueError(\"No training file uploaded. Call upload_training_file first.\")\n",
    "            \n",
    "        job = self.client.fine_tuning.jobs.create(\n",
    "            training_file=self.training_file_id,\n",
    "            validation_file = self.validation_file_id,\n",
    "            model=self.model,\n",
    "            hyperparameters={\n",
    "                \"n_epochs\": n_epochs\n",
    "            },\n",
    "            seed=1\n",
    "        )\n",
    "        \n",
    "        self.job_id = job.id\n",
    "        print(f\"Fine-tuning job created successfully. Job ID: {self.job_id}\")\n",
    "        return self.client, self.job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_folder = \"../data/cases_json/\"\n",
    "training_data_paths, validation_data_paths, test_gtpaths = sample_cases(gt_folder, n_train = 100, n_val = 10, n_test = 10, seed=2)\n",
    "\n",
    "preparer = TrainingDataPreparer()\n",
    "preparer.training_data = preparer.prepare_data(training_data_paths)\n",
    "preparer.validation_data = preparer.prepare_data(validation_data_paths)\n",
    "\n",
    "data_id = \"25.11.24_100cases\"\n",
    "preparer.save_data(data_id)\n",
    "# np.save(f\"../data/finetuning/{data_id}/test.npy\", test_gtpaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"../data/finetuning/{data_id}/train.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try to finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache Clear\n"
     ]
    }
   ],
   "source": [
    "clear_openai_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file uploaded successfully. File ID: file-7UYmyv6nZxyYKNS2RHAxfL\n",
      "Validation file uploaded successfully. File ID: file-4yQm5QHKGonqKMJFoTz45m\n"
     ]
    }
   ],
   "source": [
    "finetuner = Finetuner(model='gpt-4o-mini-2024-07-18')\n",
    "finetuner.upload_files(data_id=data_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning job created successfully. Job ID: ftjob-eboXhRujBQFOLpe4ImYLrzNH\n"
     ]
    }
   ],
   "source": [
    "client, job_id = finetuner.create_job(n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The job has successfully completed\n",
      "New fine-tuned model created\n",
      "Step 99/500: training loss=0.14\n",
      "Step 98/500: training loss=0.12\n",
      "Step 97/500: training loss=0.06\n",
      "Step 96/500: training loss=0.06\n",
      "Step 95/500: training loss=0.08\n",
      "Step 94/500: training loss=0.08\n",
      "Step 93/500: training loss=0.08\n",
      "Step 92/500: training loss=0.08\n",
      "Step 91/500: training loss=0.07\n",
      "Step 90/500: training loss=0.05, validation loss=0.05\n",
      "Step 89/500: training loss=0.09\n",
      "Step 88/500: training loss=0.13\n",
      "Step 87/500: training loss=0.13\n",
      "Step 86/500: training loss=0.07\n",
      "Step 85/500: training loss=0.17\n",
      "Step 84/500: training loss=0.12\n",
      "Step 83/500: training loss=0.08\n",
      "Step 82/500: training loss=0.12\n"
     ]
    }
   ],
   "source": [
    "for i in client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=20).data:\n",
    "    print(i.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-eboXhRujBQFOLpe4ImYLrzNH', created_at=1732553004, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:university-of-oxford::AXWYSys7', finished_at=1732553327, hyperparameters=Hyperparameters(n_epochs=5, batch_size=1, learning_rate_multiplier=1.8), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-ZqpsyAHhGKahLCkPupZx6uhn', result_files=[], seed=1, status='succeeded', trained_tokens=3709770, training_file='file-7UYmyv6nZxyYKNS2RHAxfL', validation_file='file-4yQm5QHKGonqKMJFoTz45m', estimated_finish=1732554201, integrations=[], user_provided_suffix=None)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_job = client.fine_tuning.jobs.retrieve(job_id)\n",
    "ft_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"../models/{job_id}.npy\", job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.646550000000005"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_job.trained_tokens * 3 * (1/1e6) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "law",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
